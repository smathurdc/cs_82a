{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Assignment\n",
    "## Cliff Walking with Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    ">**Make sure** you include your name along with the name of your team and team members in the notebook you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this challenge you will apply several reinforcement learning algorithms to a classic problem in reinforcement learning, known as the cliff walking problem. The cliff walking problem is basically a game. The goal is for the agent to find the highest reward (lowest cost) path from a starting state to the goal. \n",
    "\n",
    "There are a number of versions of the cliff walking problems which have been used as research benchmarks over the years. A typical cliff walking problem might use a grid of 4x12. For this challenge you will work with a reduced size grid world of 4x4 illustrated below to reduce training time for your models.   \n",
    "\n",
    "<img src=\"CliffWalking.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for similified cliff walking problem** </center>\n",
    "\n",
    "The goal is to find the highest reward path from the **starting state**, 12, to the **terminal state**, 15, making this an **episodic task**. The rewards for this task are:\n",
    "1. A reward of -1 for most state transitions. The -1 reward apples to state to state transitions and to transitions toward the boundary of the grid transitioning to the same state.    \n",
    "2. A reward of -100 for 'falling off the cliff'. Falling off the cliff occurs when entering states 13 or 14. The only possible transition out of the cliff states is back to the origin state, 12. There are no possible transitions toward the boundary from the cliff state. \n",
    "\n",
    "Intuitively, we can see that the optimal solution follows the dotted line path shown in the diagram above. The challenge is to find a path that is as close to this optimal as possible.   \n",
    "\n",
    "You can find a short discussion of the cliff walking problem on page 132 of Sutton and Barto, second edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "For this challenge you will do the following:\n",
    "\n",
    "1. Create a simulator for the grid world environment. All interactions between your agents and the environment must be though calls to the function you create.  \n",
    "2. Create and apply state value estimation agents using the RL algorithms.\n",
    "3. Use the general policy improvement (GPI) algorithm with the appropriate control algorithm to improve policy. \n",
    "4. Use the state value estimation agent to evaluate the improved policy. \n",
    "5. Compare the results for the various control algorithms you try. \n",
    "\n",
    "Methods to use to solve this problem:\n",
    "\n",
    "1. The Monte Carlo method for value estimation and (action value) control. The action value method for Monte Carlo has not been explicitly addressed in this course. You can find the pseudo code for Monte Carlo control on page 101 of Sutton and Barto, second edition.   \n",
    "2. Create and execute agents using the n-step TD method for value estimation and n-step SARSA (action value) for control.\n",
    "3. Create and execute agents using TD(0) for value estimation and SARSA(0) or Double Q-Learning (action value) control. You are welcome to try both algorithms if you have the time. \n",
    "4. For additional, but optional, challenge you may wish to try a dynamic programming algorithm. Does DP work for this problem or not, and why? \n",
    "\n",
    "> **Hints**\n",
    "> - For TD(0), n-step TD, n-step SARSA, SARSA(0) and Double Q-learning, you may need to change the reward to -10 for state transitions toward the boundary of the grid world.  \n",
    "> - For the n-step algorithms keep in mind that the grid world is rather small. \n",
    "> - Make sure you are not accidentally using two epsilon greedy steps in your GPI process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.The Monte Carlo method for value estimation and (action value) control. The action value method for Monte Carlo has not been explicitly addressed in this course. You can find the pseudo code for Monte Carlo control on page 101 of Sutton and Barto, second edition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors={0:{'u':0, 'd':4, 'l':0, 'r':1},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "          14:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "          15:{'u':15, 'd':15, 'l':15, 'r':15}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy =               {0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.00, 'd':0.00, 'l':0.00, 'r':0.00}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards ={0:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          1:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          2:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          3:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          4:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          5:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          6:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          7:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          8:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          9:{'u':-1.0, 'd':-100.0, 'l':-1.0, 'r':-1.0},\n",
    "          10:{'u':-1.0, 'd':-100.0, 'l':-1.0, 'r':-1.0},\n",
    "          11:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          12:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-100.0},\n",
    "          13:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          14:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          15:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rewards with a -10.0 for hitting the wall\n",
    "rew_wall={0:{'u':-10.0, 'd':-1.0, 'l':-10.0, 'r':-1.0},\n",
    "          1:{'u':-10.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          2:{'u':-10.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          3:{'u':-10.0, 'd':-1.0, 'l':-1.0, 'r':-10.0},\n",
    "          4:{'u':-1.0, 'd':-1.0, 'l':-10.0, 'r':-1.0},\n",
    "          5:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          6:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          7:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-10.0},\n",
    "          8:{'u':-1.0, 'd':-1.0, 'l':-10.0, 'r':-1.0},\n",
    "          9:{'u':-1.0, 'd':-100.0, 'l':-1.0, 'r':-1.0},\n",
    "          10:{'u':-1.0, 'd':-100.0, 'l':-1.0, 'r':-1.0},\n",
    "          11:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-10.0},\n",
    "          12:{'u':-1.0, 'd':-10.0, 'l':-10.0, 'r':-100.0},\n",
    "          13:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          14:{'u':-1.0, 'd':-1.0, 'l':-1.0, 'r':-1.0},\n",
    "          15:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_generate_episode(start_state, policy, neighbors, terminal):\n",
    "    ## List of states which might be visited in episode\n",
    "    n_states = len(policy)\n",
    "#    visited_state = [0] * n_states\n",
    "    states = list(neighbors.keys())\n",
    "    current_state = start_state\n",
    "    #while(current_state == terminal): # Keep trying to not use terminal state to start\n",
    "    #    current_state = nr.choice(states, size = 1)[0]\n",
    "            \n",
    "    ## Take a random walk trough the states until we get to the terminal state\n",
    "    ## We do some bookkeeping to ensure we only visit states once.\n",
    "    visited = [] # List of states visited on random walk\n",
    "    while(current_state != terminal): # Stop when at terminal state\n",
    "        ## Probability of state transition given policy\n",
    "        probs = list(policy[current_state].values())\n",
    "        ## Find next state to transition to\n",
    "        next_state = nr.choice(list(neighbors[current_state].values()), size = 1, p = probs)[0]\n",
    "        visited.append(next_state)\n",
    "        current_state = next_state  \n",
    "    return(visited)    \n",
    "    \n",
    "    \n",
    "#MC_generate_episode(start_state, policy, neighbors, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.0126417 , -7.87805027, -7.81337851, -7.67918882],\n",
       "       [-8.04466009, -8.14590834, -7.98329222, -8.26967488],\n",
       "       [-8.28342024, -8.9028726 , -9.06062143, -8.40720391],\n",
       "       [-9.33705457, -8.66709983, -8.37936446,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MC_state_values(start_state, policy, neighbors, rewards, terminal, episodes = 1):\n",
    "    '''Function for first visit Monte Carlo on GridWorld.'''\n",
    "    ## Create list of states \n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((episodes,n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(episodes):\n",
    "        ## For each episode we use a list to keep track of states we have visited.\n",
    "        ## Once we visit a state we need to accumulate values to get the returns\n",
    "        states_visited = []\n",
    "   \n",
    "        ## Get a path for this episode\n",
    "        visit_list = MC_generate_episode(start_state, policy, neighbors, terminal)\n",
    "        current_state = visit_list[0]\n",
    "        for state in visit_list[0:]: \n",
    "            ## list of states we can transition to from current state\n",
    "            transition_list = list(neighbors[current_state].values())\n",
    "            \n",
    "            if(state in transition_list): # Make sure the transistion is allowed\n",
    "                transition_index = transition_list.index(state)   \n",
    "  \n",
    "                ## find the action value for the state transition\n",
    "                v_s = list(rewards[current_state].values())[transition_index]\n",
    "   \n",
    "                ## Mark that the current state has been visited \n",
    "                if(state not in states_visited): states_visited.append(current_state)  \n",
    "                ## Loop over the states already visited to add the value to the return\n",
    "                for visited in states_visited:\n",
    "                    G[i,visited] = G[i,visited] + v_s\n",
    "                    n_visits[visited] = n_visits[visited] + 1.0\n",
    "            ## Update the current state for next transition\n",
    "            current_state = state   \n",
    "    \n",
    "    ## Compute the average of G over the episodes are return\n",
    "    n_visits = [nv if nv != 0.0 else 1.0 for nv in n_visits]\n",
    "    returns = np.divide(np.sum(G, axis = 0), n_visits)   \n",
    "    return(returns)              \n",
    "    \n",
    "returns = MC_state_values(start_state, policy, neighbors, rewards, terminal = 15, episodes = 500)\n",
    "np.array(returns).reshape((4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.7,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 1: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 2: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 3: {'d': 0.10000000000000002,\n",
       "  'l': 0.7,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 4: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 5: {'d': 0.10000000000000002,\n",
       "  'l': 0.7,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 6: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 7: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 8: {'d': 0.10000000000000002,\n",
       "  'l': 0.7,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.10000000000000002},\n",
       " 9: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 10: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.7,\n",
       "  'u': 0.10000000000000002},\n",
       " 11: {'d': 0.30099999999999993, 'l': 0.233, 'r': 0.233, 'u': 0.233},\n",
       " 12: {'d': 0.10000000000000002,\n",
       "  'l': 0.10000000000000002,\n",
       "  'r': 0.10000000000000002,\n",
       "  'u': 0.7},\n",
       " 13: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 14: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 15: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def MC_optimal_policy(start_state, policy, neighbors, rewards, terminal, episodes = 10, cycles = 10, epsilon = 0.05):\n",
    "    ## Create a working cooy of the initial policy\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    \n",
    "    ## Loop over a number of cycles of GPI\n",
    "    for _ in range(cycles):\n",
    "        ## First compute the average returns for each of the states. \n",
    "        ## This is the policy evaluation phase\n",
    "        returns = MC_state_values(start_state, current_policy, neighbors, rewards, terminal = terminal,\\\n",
    "                                  episodes = episodes)\n",
    "        \n",
    "        ## We want max Q for each state, where Q is just the difference \n",
    "        ## in the values of the possible state transition\n",
    "        ## This is the policy evaluation phase\n",
    "        for s in current_policy.keys(): # iterate over all states\n",
    "            ## Compute Q for each possible state transistion\n",
    "            ## Start by creating a list of the adjacent states.\n",
    "            possible_s_prime = neighbors[s]\n",
    "            neighbor_states = list(possible_s_prime.values())\n",
    "            ## Check if terminal state is neighbor, but state is not terminal.\n",
    "            if(terminal in neighbor_states and s != terminal):\n",
    "                ## account for the special case adjacent to goal\n",
    "                neighbor_Q = []\n",
    "                for s_prime in possible_s_prime.keys(): # Iterate over adjacent states\n",
    "                    if(neighbors[s][s_prime] == terminal):  \n",
    "                         neighbor_Q.append(returns[s])\n",
    "                    else: neighbor_Q.append(0.0) ## Other transisions have 0 value.   \n",
    "            else: \n",
    "                 ## The other case is rather easy. Compute Q for the transistion to each neighbor           \n",
    "                 neighbor_values = returns[neighbor_states]\n",
    "                 neighbor_Q = [n_val - returns[s] for n_val in neighbor_values]\n",
    "                \n",
    "            ## Find the index for the state transistions with the largest values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(np.array(neighbor_Q) == max(neighbor_Q))[0]  \n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(len(np.array(neighbor_Q)))\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder          \n",
    "                   \n",
    "    return current_policy\n",
    " \n",
    " \n",
    "nr.seed(9876)\n",
    "MC_policy = MC_optimal_policy(start_state, policy, neighbors, rew_wall, terminal = 15, episodes = 50, cycles = 10, \n",
    "                              epsilon = 0.1)  \n",
    "MC_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Create a simulator for the grid world environment. All interactions between your agents and the environment must be though calls to the function you create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(start_state, policy, neighbors, rewards, terminal):\n",
    "    s = list(policy[start_state].keys())\n",
    "    k = nr.choice(s, size = 1)[0]\n",
    "    r = rew_wall[start_state][k]\n",
    "    s_next = neighbors[start_state][k]\n",
    "    done = False\n",
    "    if s_next == terminal:\n",
    "        done = True\n",
    "    return (start_state,s_next,k,r,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  8 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  8 Next State =  12 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  12 Next State =  13 Action =  r Reward =  -100.0 Done =  False\n",
      "Current State =  13 Next State =  12 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  8 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  8 Next State =  4 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  4 Next State =  4 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  4 Next State =  5 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  5 Next State =  6 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  6 Next State =  5 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  5 Next State =  9 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  9 Next State =  10 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  10 Next State =  14 Action =  d Reward =  -100.0 Done =  False\n",
      "Current State =  14 Next State =  12 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  13 Action =  r Reward =  -100.0 Done =  False\n",
      "Current State =  13 Next State =  12 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  8 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  8 Next State =  9 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  9 Next State =  5 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  5 Next State =  9 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  9 Next State =  13 Action =  d Reward =  -100.0 Done =  False\n",
      "Current State =  13 Next State =  12 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  8 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  8 Next State =  9 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  9 Next State =  10 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  10 Next State =  9 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  9 Next State =  13 Action =  d Reward =  -100.0 Done =  False\n",
      "Current State =  13 Next State =  12 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  d Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  12 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  12 Next State =  8 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  8 Next State =  9 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  9 Next State =  5 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  5 Next State =  1 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  0 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  4 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  4 Next State =  0 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  1 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  0 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  0 Next State =  4 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  4 Next State =  5 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  5 Next State =  1 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  2 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  2 Next State =  1 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  2 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  2 Next State =  1 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  0 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  1 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  0 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  l Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  0 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  0 Next State =  1 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  0 Action =  l Reward =  -1.0 Done =  False\n",
      "Current State =  0 Next State =  1 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  1 Next State =  2 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  2 Next State =  2 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  2 Next State =  2 Action =  u Reward =  -10.0 Done =  False\n",
      "Current State =  2 Next State =  6 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  6 Next State =  2 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  2 Next State =  3 Action =  r Reward =  -1.0 Done =  False\n",
      "Current State =  3 Next State =  7 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  7 Next State =  11 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  11 Next State =  7 Action =  u Reward =  -1.0 Done =  False\n",
      "Current State =  7 Next State =  7 Action =  r Reward =  -10.0 Done =  False\n",
      "Current State =  7 Next State =  11 Action =  d Reward =  -1.0 Done =  False\n",
      "Current State =  11 Next State =  15 Action =  d Reward =  -1.0 Done =  True\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "start_state = 12\n",
    "while done == False:\n",
    "    (start_state,s_next,action,r,done) = next_state(start_state, policy, neighbors, rew_wall, 15)\n",
    "    print(\"Current State = \",start_state,\"Next State = \",s_next,\"Action = \",action,\"Reward = \",r,\"Done = \",done)\n",
    "    start_state = s_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov - Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "def mc_control(start_state, policy, neighbors, rewards, terminal, cycles, episodes,epsilon):\n",
    "    cp = copy.deepcopy(policy)\n",
    "    \n",
    "    for j in range(0,cycles):\n",
    "        nx = []\n",
    "        act = []\n",
    "        rew = []\n",
    "        done = False\n",
    "    \n",
    "        for z in range(0,episodes): \n",
    "            done = False\n",
    "            start_state = 12\n",
    "            while done == False:\n",
    "                (current_state,s_next,action,r,done) = next_state(start_state, policy, neighbors, rew_wall, 15)\n",
    "                start_state = s_next\n",
    "                nx.append(current_state)\n",
    "                act.append(action)\n",
    "                rew.append(r)\n",
    "        df = pd.DataFrame(OrderedDict({'state':nx, 'action':act, 'reward':rew}))\n",
    "        f1 = df.groupby(['state','action']).mean()\n",
    "        f1.reset_index(inplace=True)\n",
    "        max_df = f1.groupby(['state','action']).max()\n",
    "        #f2 = max_df.reset_index()\n",
    "        to_be_merged = max_df.groupby(['state']).max()\n",
    "        to_be_merged.reset_index(inplace=True)\n",
    "        f2 = f1.merge(to_be_merged, on=['state'])\n",
    "        f3 = f2[f2.reward_x == f2.reward_y]\n",
    "        #df = f3\n",
    "        p = random.random()\n",
    "        if p >0.5:\n",
    "            df = f3.groupby('state').last().reset_index()\n",
    "        else:\n",
    "            df = f3.groupby('state').first().reset_index()\n",
    "        #df = f3.groupby('state').sample(n=1).reset_index()\n",
    "        #return(df)\n",
    "        \n",
    "        #Update Policy\n",
    "        st = df.index.tolist()\n",
    "        action = df.action.tolist()\n",
    "\n",
    "        prob_for_policy = 1- epsilon\n",
    "        remainder = epsilon\n",
    "        for k in range(0,len(st)):\n",
    "            for i, key in enumerate(cp[st[k]]): ## Update policy\n",
    "                #print(i,key)\n",
    "                if key == action[k]:\n",
    "                    #cp[st[k]][key] = 1 - epsilon + (epsilon/np.absolute(r[k]))\n",
    "                    cp[st[k]][key] = 0.7\n",
    "                else:\n",
    "                    #cp[st[k]][key] = epsilon\n",
    "                    cp[st[k]][key] = 0.1\n",
    "    \n",
    "    \n",
    "    return(cp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.1, 'l': 0.1, 'r': 0.7, 'u': 0.1},\n",
       " 1: {'d': 0.1, 'l': 0.1, 'r': 0.7, 'u': 0.1},\n",
       " 2: {'d': 0.1, 'l': 0.1, 'r': 0.7, 'u': 0.1},\n",
       " 3: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 4: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 5: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 6: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 7: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 8: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 9: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 10: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 11: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 12: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 13: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 14: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 15: {'d': 0.0, 'l': 0.0, 'r': 0.0, 'u': 0.0}}"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = mc_control(12, policy, neighbors, rewards, 15, 20,1000,0.1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create and execute agents using the n-step TD method for value estimation and n-step SARSA (action value) for control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
